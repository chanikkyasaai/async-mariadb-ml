{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828a6a75",
   "metadata": {},
   "source": [
    "# RAG with MariaDB: Async Vector Storage and Retrieval\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using:\n",
    "- **async-mariadb-connector** for fast async database operations\n",
    "- **MariaDB** for storing document embeddings\n",
    "- **LangChain** for RAG orchestration\n",
    "\n",
    "## Why MariaDB for RAG?\n",
    "- ‚úÖ Native JSON support for metadata\n",
    "- ‚úÖ Efficient vector storage (JSON arrays or BLOB)\n",
    "- ‚úÖ ACID transactions for data integrity\n",
    "- ‚úÖ Mature, production-ready database\n",
    "- ‚úÖ Great for hybrid search (vector + full-text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa7ec8",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c69b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install async-mariadb-connector\n",
    "# !pip install langchain langchain-openai\n",
    "# !pip install numpy pandas\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from async_mariadb_connector import AsyncMariaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0584dad",
   "metadata": {},
   "source": [
    "## 2. Create Vector Store Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_vector_store():\n",
    "    \"\"\"\n",
    "    Create a table for storing document embeddings.\n",
    "    Uses JSON for storing vectors and metadata.\n",
    "    \"\"\"\n",
    "    db = AsyncMariaDB()\n",
    "    \n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS document_embeddings (\n",
    "        id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        document_id VARCHAR(255) NOT NULL,\n",
    "        content TEXT NOT NULL,\n",
    "        embedding JSON NOT NULL,\n",
    "        metadata JSON,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        INDEX idx_doc_id (document_id),\n",
    "        FULLTEXT INDEX ft_content (content)\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n",
    "    \"\"\"\n",
    "    \n",
    "    await db.execute(create_table_sql)\n",
    "    print(\"‚úÖ Vector store table created\")\n",
    "    \n",
    "    await db.close()\n",
    "\n",
    "# Run setup\n",
    "await setup_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808afe5a",
   "metadata": {},
   "source": [
    "## 3. MariaDB Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MariaDBVectorStore:\n",
    "    \"\"\"\n",
    "    Async vector store implementation using MariaDB.\n",
    "    Compatible with LangChain's VectorStore interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.db = AsyncMariaDB()\n",
    "    \n",
    "    async def add_documents(\n",
    "        self,\n",
    "        documents: List[str],\n",
    "        embeddings: List[List[float]],\n",
    "        metadatas: Optional[List[Dict[str, Any]]] = None,\n",
    "        document_ids: Optional[List[str]] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Add documents with their embeddings to the store.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document texts\n",
    "            embeddings: List of embedding vectors\n",
    "            metadatas: Optional metadata for each document\n",
    "            document_ids: Optional IDs for documents\n",
    "        \n",
    "        Returns:\n",
    "            List of inserted document IDs\n",
    "        \"\"\"\n",
    "        if metadatas is None:\n",
    "            metadatas = [{} for _ in documents]\n",
    "        \n",
    "        if document_ids is None:\n",
    "            document_ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "        \n",
    "        insert_sql = \"\"\"\n",
    "            INSERT INTO document_embeddings \n",
    "            (document_id, content, embedding, metadata)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare data for batch insert\n",
    "        data = [\n",
    "            (\n",
    "                doc_id,\n",
    "                content,\n",
    "                json.dumps(embedding),  # Store as JSON array\n",
    "                json.dumps(metadata)\n",
    "            )\n",
    "            for doc_id, content, embedding, metadata\n",
    "            in zip(document_ids, documents, embeddings, metadatas)\n",
    "        ]\n",
    "        \n",
    "        await self.db.execute_many(insert_sql, data)\n",
    "        print(f\"‚úÖ Added {len(documents)} documents to vector store\")\n",
    "        \n",
    "        return document_ids\n",
    "    \n",
    "    async def similarity_search(\n",
    "        self,\n",
    "        query_embedding: List[float],\n",
    "        k: int = 4\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Find k most similar documents using cosine similarity.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: Query vector\n",
    "            k: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of documents with similarity scores\n",
    "        \"\"\"\n",
    "        # Fetch all embeddings (for large datasets, implement pagination)\n",
    "        fetch_sql = \"SELECT id, document_id, content, embedding, metadata FROM document_embeddings\"\n",
    "        results = await self.db.fetch_all(fetch_sql)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        query_vec = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        \n",
    "        for row in results:\n",
    "            embedding = json.loads(row['embedding'])\n",
    "            doc_vec = np.array(embedding)\n",
    "            \n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(query_vec, doc_vec) / (\n",
    "                np.linalg.norm(query_vec) * np.linalg.norm(doc_vec)\n",
    "            )\n",
    "            \n",
    "            similarities.append({\n",
    "                'id': row['id'],\n",
    "                'document_id': row['document_id'],\n",
    "                'content': row['content'],\n",
    "                'metadata': json.loads(row['metadata']) if row['metadata'] else {},\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "        \n",
    "        # Sort by similarity and return top k\n",
    "        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return similarities[:k]\n",
    "    \n",
    "    async def hybrid_search(\n",
    "        self,\n",
    "        query_text: str,\n",
    "        query_embedding: List[float],\n",
    "        k: int = 4,\n",
    "        text_weight: float = 0.3,\n",
    "        vector_weight: float = 0.7\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Hybrid search combining full-text and vector similarity.\n",
    "        \n",
    "        Args:\n",
    "            query_text: Text query for full-text search\n",
    "            query_embedding: Vector for semantic search\n",
    "            k: Number of results\n",
    "            text_weight: Weight for full-text score\n",
    "            vector_weight: Weight for vector similarity\n",
    "        \n",
    "        Returns:\n",
    "            List of documents with combined scores\n",
    "        \"\"\"\n",
    "        # Get full-text search results\n",
    "        fts_sql = \"\"\"\n",
    "            SELECT id, document_id, content, embedding, metadata,\n",
    "                   MATCH(content) AGAINST(%s IN NATURAL LANGUAGE MODE) as text_score\n",
    "            FROM document_embeddings\n",
    "            WHERE MATCH(content) AGAINST(%s IN NATURAL LANGUAGE MODE)\n",
    "        \"\"\"\n",
    "        fts_results = await self.db.fetch_all(fts_sql, (query_text, query_text))\n",
    "        \n",
    "        # Get vector similarity results\n",
    "        vector_results = await self.similarity_search(query_embedding, k=k*2)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined = {}\n",
    "        \n",
    "        # Add full-text scores\n",
    "        for row in fts_results:\n",
    "            doc_id = row['id']\n",
    "            combined[doc_id] = {\n",
    "                'id': row['id'],\n",
    "                'document_id': row['document_id'],\n",
    "                'content': row['content'],\n",
    "                'metadata': json.loads(row['metadata']) if row['metadata'] else {},\n",
    "                'text_score': float(row['text_score']),\n",
    "                'vector_score': 0.0\n",
    "            }\n",
    "        \n",
    "        # Add vector scores\n",
    "        for result in vector_results:\n",
    "            doc_id = result['id']\n",
    "            if doc_id in combined:\n",
    "                combined[doc_id]['vector_score'] = result['similarity']\n",
    "            else:\n",
    "                combined[doc_id] = {\n",
    "                    **result,\n",
    "                    'text_score': 0.0,\n",
    "                    'vector_score': result['similarity']\n",
    "                }\n",
    "        \n",
    "        # Calculate combined score\n",
    "        for doc in combined.values():\n",
    "            doc['combined_score'] = (\n",
    "                text_weight * doc['text_score'] +\n",
    "                vector_weight * doc['vector_score']\n",
    "            )\n",
    "        \n",
    "        # Sort and return top k\n",
    "        results = sorted(\n",
    "            combined.values(),\n",
    "            key=lambda x: x['combined_score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        return results[:k]\n",
    "    \n",
    "    async def delete_documents(self, document_ids: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        Delete documents by their IDs.\n",
    "        \n",
    "        Args:\n",
    "            document_ids: List of document IDs to delete\n",
    "        \n",
    "        Returns:\n",
    "            Number of documents deleted\n",
    "        \"\"\"\n",
    "        placeholders = ','.join(['%s'] * len(document_ids))\n",
    "        delete_sql = f\"DELETE FROM document_embeddings WHERE document_id IN ({placeholders})\"\n",
    "        \n",
    "        result = await self.db.execute(delete_sql, tuple(document_ids))\n",
    "        return result\n",
    "    \n",
    "    async def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        await self.db.close()\n",
    "\n",
    "print(\"‚úÖ MariaDBVectorStore class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9ef61",
   "metadata": {},
   "source": [
    "## 4. Example: Add Documents to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85943a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def add_sample_documents():\n",
    "    \"\"\"Add sample documents with mock embeddings.\"\"\"\n",
    "    \n",
    "    # Sample documents about MariaDB and databases\n",
    "    documents = [\n",
    "        \"MariaDB is a community-developed, commercially supported fork of MySQL.\",\n",
    "        \"MariaDB supports JSON data types for storing semi-structured data.\",\n",
    "        \"Vector databases are optimized for similarity search using embeddings.\",\n",
    "        \"RAG systems combine retrieval and generation for better LLM responses.\",\n",
    "        \"Async database operations improve performance in I/O-bound applications.\",\n",
    "        \"MariaDB offers full-text search capabilities for text indexing.\",\n",
    "        \"Embeddings are dense vector representations of text or other data.\",\n",
    "        \"LangChain simplifies building applications with language models.\",\n",
    "    ]\n",
    "    \n",
    "    # Generate mock embeddings (in production, use actual embedding model)\n",
    "    # For example: OpenAI's text-embedding-ada-002 or sentence-transformers\n",
    "    np.random.seed(42)\n",
    "    embeddings = [np.random.randn(384).tolist() for _ in documents]\n",
    "    \n",
    "    # Metadata for each document\n",
    "    metadatas = [\n",
    "        {\"category\": \"database\", \"topic\": \"mariadb\"},\n",
    "        {\"category\": \"database\", \"topic\": \"json\"},\n",
    "        {\"category\": \"ai\", \"topic\": \"vectors\"},\n",
    "        {\"category\": \"ai\", \"topic\": \"rag\"},\n",
    "        {\"category\": \"programming\", \"topic\": \"async\"},\n",
    "        {\"category\": \"database\", \"topic\": \"search\"},\n",
    "        {\"category\": \"ai\", \"topic\": \"embeddings\"},\n",
    "        {\"category\": \"ai\", \"topic\": \"langchain\"},\n",
    "    ]\n",
    "    \n",
    "    vector_store = MariaDBVectorStore()\n",
    "    \n",
    "    # Add documents\n",
    "    doc_ids = await vector_store.add_documents(\n",
    "        documents=documents,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìù Added documents with IDs: {doc_ids}\")\n",
    "    \n",
    "    await vector_store.close()\n",
    "\n",
    "await add_sample_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6e02e",
   "metadata": {},
   "source": [
    "## 5. Example: Vector Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_similarity_search():\n",
    "    \"\"\"Test vector similarity search.\"\"\"\n",
    "    \n",
    "    vector_store = MariaDBVectorStore()\n",
    "    \n",
    "    # Mock query embedding (in production, embed the query text)\n",
    "    np.random.seed(42)\n",
    "    query_embedding = np.random.randn(384).tolist()\n",
    "    \n",
    "    print(\"üîç Searching for similar documents...\\n\")\n",
    "    \n",
    "    results = await vector_store.similarity_search(\n",
    "        query_embedding=query_embedding,\n",
    "        k=3\n",
    "    )\n",
    "    \n",
    "    print(\"Top 3 Most Similar Documents:\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. Similarity: {result['similarity']:.4f}\")\n",
    "        print(f\"   Content: {result['content']}\")\n",
    "        print(f\"   Metadata: {result['metadata']}\")\n",
    "        print()\n",
    "    \n",
    "    await vector_store.close()\n",
    "\n",
    "await test_similarity_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fac98",
   "metadata": {},
   "source": [
    "## 6. Example: Hybrid Search (Full-Text + Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_hybrid_search():\n",
    "    \"\"\"Test hybrid search combining full-text and vector similarity.\"\"\"\n",
    "    \n",
    "    vector_store = MariaDBVectorStore()\n",
    "    \n",
    "    query_text = \"MariaDB database features\"\n",
    "    \n",
    "    # Mock query embedding\n",
    "    np.random.seed(123)\n",
    "    query_embedding = np.random.randn(384).tolist()\n",
    "    \n",
    "    print(f\"üîç Hybrid Search Query: '{query_text}'\\n\")\n",
    "    \n",
    "    results = await vector_store.hybrid_search(\n",
    "        query_text=query_text,\n",
    "        query_embedding=query_embedding,\n",
    "        k=3,\n",
    "        text_weight=0.4,\n",
    "        vector_weight=0.6\n",
    "    )\n",
    "    \n",
    "    print(\"Top 3 Hybrid Search Results:\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n",
    "        print(f\"   Text Score: {result['text_score']:.4f}\")\n",
    "        print(f\"   Vector Score: {result['vector_score']:.4f}\")\n",
    "        print(f\"   Content: {result['content']}\")\n",
    "        print()\n",
    "    \n",
    "    await vector_store.close()\n",
    "\n",
    "await test_hybrid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c498c",
   "metadata": {},
   "source": [
    "## 7. RAG Pipeline: Retrieval + Generation\n",
    "\n",
    "This example shows how to integrate the vector store with an LLM for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14426d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rag_pipeline_example():\n",
    "    \"\"\"\n",
    "    Example RAG pipeline: Retrieve relevant docs and generate answer.\n",
    "    \n",
    "    Note: This is a mock example. In production:\n",
    "    1. Use a real embedding model (OpenAI, HuggingFace, etc.)\n",
    "    2. Use a real LLM for generation (GPT-4, Claude, Llama, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    vector_store = MariaDBVectorStore()\n",
    "    \n",
    "    # User question\n",
    "    question = \"What is MariaDB good for?\"\n",
    "    \n",
    "    print(f\"‚ùì Question: {question}\\n\")\n",
    "    \n",
    "    # Step 1: Generate query embedding (mock)\n",
    "    np.random.seed(42)\n",
    "    query_embedding = np.random.randn(384).tolist()\n",
    "    \n",
    "    # Step 2: Retrieve relevant documents\n",
    "    print(\"üìö Retrieving relevant documents...\\n\")\n",
    "    relevant_docs = await vector_store.similarity_search(\n",
    "        query_embedding=query_embedding,\n",
    "        k=3\n",
    "    )\n",
    "    \n",
    "    # Step 3: Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}: {doc['content']}\"\n",
    "        for i, doc in enumerate(relevant_docs)\n",
    "    ])\n",
    "    \n",
    "    print(\"Retrieved Context:\")\n",
    "    print(context)\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Generate answer using LLM (mock)\n",
    "    # In production, use:\n",
    "    # from langchain_openai import ChatOpenAI\n",
    "    # llm = ChatOpenAI(model=\"gpt-4\")\n",
    "    # answer = llm.invoke(prompt)\n",
    "    \n",
    "    mock_answer = (\n",
    "        \"Based on the retrieved documents, MariaDB is a community-developed fork of MySQL \"\n",
    "        \"that supports JSON data types for semi-structured data and offers full-text search \"\n",
    "        \"capabilities. It's well-suited for applications requiring both traditional relational \"\n",
    "        \"database features and modern data handling capabilities.\"\n",
    "    )\n",
    "    \n",
    "    print(\"ü§ñ Generated Answer:\")\n",
    "    print(mock_answer)\n",
    "    print()\n",
    "    \n",
    "    await vector_store.close()\n",
    "    \n",
    "    print(\"\\n‚úÖ RAG pipeline completed!\")\n",
    "    print(\"\\nTo use with real LLM:\")\n",
    "    print(\"  1. Install: pip install langchain-openai\")\n",
    "    print(\"  2. Set OPENAI_API_KEY environment variable\")\n",
    "    print(\"  3. Replace mock LLM with: ChatOpenAI(model='gpt-4')\")\n",
    "\n",
    "await rag_pipeline_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c699676",
   "metadata": {},
   "source": [
    "## 8. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a88567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete all documents\n",
    "# async def cleanup():\n",
    "#     db = AsyncMariaDB()\n",
    "#     await db.execute(\"TRUNCATE TABLE document_embeddings\")\n",
    "#     await db.close()\n",
    "#     print(\"üóëÔ∏è All documents deleted\")\n",
    "# \n",
    "# await cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf89018",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ‚úÖ **Setting up MariaDB for vector storage** - JSON columns for embeddings\n",
    "2. ‚úÖ **Async vector store implementation** - Compatible with LangChain\n",
    "3. ‚úÖ **Similarity search** - Cosine similarity for semantic retrieval\n",
    "4. ‚úÖ **Hybrid search** - Combining full-text and vector search\n",
    "5. ‚úÖ **RAG pipeline** - Retrieval + Generation workflow\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "- **Embeddings**: Use real models (OpenAI, Cohere, HuggingFace)\n",
    "- **Indexing**: For large datasets, add database indexes\n",
    "- **Pagination**: Implement pagination for large result sets\n",
    "- **Caching**: Cache frequently accessed embeddings\n",
    "- **Monitoring**: Add logging and metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Install LangChain: `pip install langchain langchain-openai`\n",
    "- Get API keys for embedding models (OpenAI, Cohere, etc.)\n",
    "- Replace mock embeddings with real embedding generation\n",
    "- Deploy to production with proper connection pooling\n",
    "\n",
    "**Happy building! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
