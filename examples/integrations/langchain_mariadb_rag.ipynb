{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828a6a75",
   "metadata": {},
   "source": [
    "# RAG with MariaDB: Async Vector Storage and Retrieval\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using:\n",
    "- **async-mariadb-connector** for fast async database operations\n",
    "- **MariaDB** for storing document embeddings\n",
    "- **LangChain** for RAG orchestration\n",
    "\n",
    "## Why MariaDB for RAG?\n",
    "- ✅ Native JSON support for metadata\n",
    "- ✅ Efficient vector storage (JSON arrays or BLOB)\n",
    "- ✅ ACID transactions for data integrity\n",
    "- ✅ Mature, production-ready database\n",
    "- ✅ Great for hybrid search (vector + full-text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa7ec8",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9c69b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install async-mariadb-connector\n",
    "# !pip install langchain langchain-openai\n",
    "# !pip install numpy pandas\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from async_mariadb_connector import AsyncMariaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0584dad",
   "metadata": {},
   "source": [
    "## 2. Create Vector Store Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bffa28a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 21:39:07,657 - async_mariadb_connector.core - INFO - Creating database connection pool.\n",
      "2025-10-21 21:39:07,659 - async_mariadb_connector.core - INFO - Database connection pool closed.\n",
      "2025-10-21 21:39:07,659 - async_mariadb_connector.core - INFO - Database connection pool closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store table created\n"
     ]
    }
   ],
   "source": [
    "async def setup_vector_store():\n",
    "    \"\"\"\n",
    "    Create a table for storing document embeddings.\n",
    "    Uses JSON for storing vectors and metadata.\n",
    "    \"\"\"\n",
    "    db = AsyncMariaDB()\n",
    "    \n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS document_embeddings (\n",
    "        id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        document_id VARCHAR(255) NOT NULL,\n",
    "        content TEXT NOT NULL,\n",
    "        embedding JSON NOT NULL,\n",
    "        metadata JSON,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        INDEX idx_doc_id (document_id),\n",
    "        FULLTEXT INDEX ft_content (content)\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n",
    "    \"\"\"\n",
    "    \n",
    "    await db.execute(create_table_sql)\n",
    "    print(\"✅ Vector store table created\")\n",
    "    \n",
    "    await db.close()\n",
    "\n",
    "# Run setup\n",
    "await setup_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808afe5a",
   "metadata": {},
   "source": [
    "## 3. MariaDB Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daff5cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MariaDBVectorStore class defined\n"
     ]
    }
   ],
   "source": [
    "class MariaDBVectorStore:\n",
    "    \"\"\"\n",
    "    Async vector store implementation using MariaDB.\n",
    "    Compatible with LangChain's VectorStore interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.db = AsyncMariaDB()\n",
    "    \n",
    "    async def add_documents(\n",
    "        self,\n",
    "        documents: List[str],\n",
    "        embeddings: List[List[float]],\n",
    "        metadatas: Optional[List[Dict[str, Any]]] = None,\n",
    "        document_ids: Optional[List[str]] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Add documents with their embeddings to the store.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document texts\n",
    "            embeddings: List of embedding vectors\n",
    "            metadatas: Optional metadata for each document\n",
    "            document_ids: Optional IDs for documents\n",
    "        \n",
    "        Returns:\n",
    "            List of inserted document IDs\n",
    "        \"\"\"\n",
    "        if metadatas is None:\n",
    "            metadatas = [{} for _ in documents]\n",
    "        \n",
    "        if document_ids is None:\n",
    "            document_ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "        \n",
    "        insert_sql = \"\"\"\n",
    "            INSERT INTO document_embeddings \n",
    "            (document_id, content, embedding, metadata)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare data for batch insert\n",
    "        data = [\n",
    "            (\n",
    "                doc_id,\n",
    "                content,\n",
    "                json.dumps(embedding),  # Store as JSON array\n",
    "                json.dumps(metadata)\n",
    "            )\n",
    "            for doc_id, content, embedding, metadata\n",
    "            in zip(document_ids, documents, embeddings, metadatas)\n",
    "        ]\n",
    "        \n",
    "        # Batch insert using executemany - simple and efficient!\n",
    "        await self.db.executemany(insert_sql, data)\n",
    "        print(f\"✅ Added {len(documents)} documents to vector store\")\n",
    "        \n",
    "        return document_ids\n",
    "    \n",
    "    async def similarity_search(\n",
    "        self,\n",
    "        query_embedding: List[float],\n",
    "        k: int = 4\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Find k most similar documents using cosine similarity.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: Query vector\n",
    "            k: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of documents with similarity scores\n",
    "        \"\"\"\n",
    "        # Fetch all embeddings (for large datasets, implement pagination)\n",
    "        fetch_sql = \"SELECT id, document_id, content, embedding, metadata FROM document_embeddings\"\n",
    "        results = await self.db.fetch_all(fetch_sql)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        query_vec = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        \n",
    "        for row in results:\n",
    "            embedding = json.loads(row['embedding'])\n",
    "            doc_vec = np.array(embedding)\n",
    "            \n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(query_vec, doc_vec) / (\n",
    "                np.linalg.norm(query_vec) * np.linalg.norm(doc_vec)\n",
    "            )\n",
    "            \n",
    "            similarities.append({\n",
    "                'id': row['id'],\n",
    "                'document_id': row['document_id'],\n",
    "                'content': row['content'],\n",
    "                'metadata': json.loads(row['metadata']) if row['metadata'] else {},\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "        \n",
    "        # Sort by similarity and return top k\n",
    "        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return similarities[:k]\n",
    "    \n",
    "    async def hybrid_search(\n",
    "        self,\n",
    "        query_text: str,\n",
    "        query_embedding: List[float],\n",
    "        k: int = 4,\n",
    "        text_weight: float = 0.3,\n",
    "        vector_weight: float = 0.7\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Hybrid search combining full-text and vector similarity.\n",
    "        \n",
    "        Args:\n",
    "            query_text: Text query for full-text search\n",
    "            query_embedding: Vector for semantic search\n",
    "            k: Number of results\n",
    "            text_weight: Weight for full-text score\n",
    "            vector_weight: Weight for vector similarity\n",
    "        \n",
    "        Returns:\n",
    "            List of documents with combined scores\n",
    "        \"\"\"\n",
    "        # Get full-text search results\n",
    "        fts_sql = \"\"\"\n",
    "            SELECT id, document_id, content, embedding, metadata,\n",
    "                   MATCH(content) AGAINST(%s IN NATURAL LANGUAGE MODE) as text_score\n",
    "            FROM document_embeddings\n",
    "            WHERE MATCH(content) AGAINST(%s IN NATURAL LANGUAGE MODE)\n",
    "        \"\"\"\n",
    "        fts_results = await self.db.fetch_all(fts_sql, (query_text, query_text))\n",
    "        \n",
    "        # Get vector similarity results\n",
    "        vector_results = await self.similarity_search(query_embedding, k=k*2)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined = {}\n",
    "        \n",
    "        # Add full-text scores\n",
    "        for row in fts_results:\n",
    "            doc_id = row['id']\n",
    "            combined[doc_id] = {\n",
    "                'id': row['id'],\n",
    "                'document_id': row['document_id'],\n",
    "                'content': row['content'],\n",
    "                'metadata': json.loads(row['metadata']) if row['metadata'] else {},\n",
    "                'text_score': float(row['text_score']),\n",
    "                'vector_score': 0.0\n",
    "            }\n",
    "        \n",
    "        # Add vector scores\n",
    "        for result in vector_results:\n",
    "            doc_id = result['id']\n",
    "            if doc_id in combined:\n",
    "                combined[doc_id]['vector_score'] = result['similarity']\n",
    "            else:\n",
    "                combined[doc_id] = {\n",
    "                    **result,\n",
    "                    'text_score': 0.0,\n",
    "                    'vector_score': result['similarity']\n",
    "                }\n",
    "        \n",
    "        # Calculate combined score\n",
    "        for doc in combined.values():\n",
    "            doc['combined_score'] = (\n",
    "                text_weight * doc['text_score'] +\n",
    "                vector_weight * doc['vector_score']\n",
    "            )\n",
    "        \n",
    "        # Sort and return top k\n",
    "        results = sorted(\n",
    "            combined.values(),\n",
    "            key=lambda x: x['combined_score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        return results[:k]\n",
    "    \n",
    "    async def delete_documents(self, document_ids: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        Delete documents by their IDs.\n",
    "        \n",
    "        Args:\n",
    "            document_ids: List of document IDs to delete\n",
    "        \n",
    "        Returns:\n",
    "            Number of documents deleted\n",
    "        \"\"\"\n",
    "        placeholders = ','.join(['%s'] * len(document_ids))\n",
    "        delete_sql = f\"DELETE FROM document_embeddings WHERE document_id IN ({placeholders})\"\n",
    "        \n",
    "        result = await self.db.execute(delete_sql, tuple(document_ids))\n",
    "        return result\n",
    "    \n",
    "    async def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        await self.db.close()\n",
    "\n",
    "print(\"✅ MariaDBVectorStore class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9ef61",
   "metadata": {},
   "source": [
    "## 4. Example: Add Documents to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c6fe555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 21:39:07,689 - async_mariadb_connector.core - INFO - Creating database connection pool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🧪 TESTING: executemany() - Old vs New Approach\n",
      "======================================================================\n",
      "\n",
      "📝 APPROACH 1 (OLD - Complex): Direct pool access\n",
      "----------------------------------------------------------------------\n",
      "✅ Inserted 5 rows using OLD approach\n",
      "⏱️  Time taken: 2.98ms\n",
      "📊 Lines of code: 6 (complex, exposes internals)\n",
      "\n",
      "📝 APPROACH 2 (NEW - Simple): Using executemany() method\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AsyncMariaDB' object has no attribute 'executemany'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m db.close()\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m test_executemany_approaches()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mtest_executemany_approaches\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     72\u001b[39m start = time.time()\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# THE NEW WAY - 1 simple line!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutemany\u001b[49m(insert_sql, test_data)\n\u001b[32m     77\u001b[39m elapsed2 = time.time() - start\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Verify insertion\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'AsyncMariaDB' object has no attribute 'executemany'"
     ]
    }
   ],
   "source": [
    "async def test_executemany_approaches():\n",
    "    \"\"\"\n",
    "    Test BOTH approaches to verify executemany() works correctly.\n",
    "    \n",
    "    Approach 1 (OLD - Complex): Direct pool access with 6 lines\n",
    "    Approach 2 (NEW - Simple): Using executemany() with 1 line\n",
    "    \"\"\"\n",
    "    \n",
    "    db = AsyncMariaDB()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"🧪 TESTING: executemany() - Old vs New Approach\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create test table\n",
    "    await db.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS test_batch_insert (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            name VARCHAR(100),\n",
    "            value INT\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Clear any existing data\n",
    "    await db.execute(\"TRUNCATE TABLE test_batch_insert\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_data = [\n",
    "        (\"Item A\", 100),\n",
    "        (\"Item B\", 200),\n",
    "        (\"Item C\", 300),\n",
    "        (\"Item D\", 400),\n",
    "        (\"Item E\", 500)\n",
    "    ]\n",
    "    \n",
    "    insert_sql = \"INSERT INTO test_batch_insert (name, value) VALUES (%s, %s)\"\n",
    "    \n",
    "    # ============================================================\n",
    "    # APPROACH 1 (OLD): Direct pool access - 6 LINES OF CODE\n",
    "    # ============================================================\n",
    "    print(\"\\n📝 APPROACH 1 (OLD - Complex): Direct pool access\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    # THE OLD WAY - 6 lines of boilerplate code\n",
    "    pool = await db._get_pool()\n",
    "    async with pool.acquire() as conn:\n",
    "        async with conn.cursor() as cur:\n",
    "            await cur.executemany(insert_sql, test_data)\n",
    "            if not db.db_config['autocommit']:\n",
    "                await conn.commit()\n",
    "    \n",
    "    elapsed1 = time.time() - start\n",
    "    \n",
    "    # Verify insertion\n",
    "    count1 = await db.fetch_one(\"SELECT COUNT(*) as count FROM test_batch_insert\")\n",
    "    print(f\"✅ Inserted {count1['count']} rows using OLD approach\")\n",
    "    print(f\"⏱️  Time taken: {elapsed1*1000:.2f}ms\")\n",
    "    print(f\"📊 Lines of code: 6 (complex, exposes internals)\")\n",
    "    \n",
    "    # Clear for next test\n",
    "    await db.execute(\"TRUNCATE TABLE test_batch_insert\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # APPROACH 2 (NEW): Using executemany() method - 1 LINE\n",
    "    # ============================================================\n",
    "    print(\"\\n📝 APPROACH 2 (NEW - Simple): Using executemany() method\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # THE NEW WAY - 1 simple line!\n",
    "    await db.executemany(insert_sql, test_data)\n",
    "    \n",
    "    elapsed2 = time.time() - start\n",
    "    \n",
    "    # Verify insertion\n",
    "    count2 = await db.fetch_one(\"SELECT COUNT(*) as count FROM test_batch_insert\")\n",
    "    print(f\"✅ Inserted {count2['count']} rows using NEW approach\")\n",
    "    print(f\"⏱️  Time taken: {elapsed2*1000:.2f}ms\")\n",
    "    print(f\"📊 Lines of code: 1 (simple, clean API)\")\n",
    "    \n",
    "    # Verify data integrity\n",
    "    results = await db.fetch_all(\"SELECT name, value FROM test_batch_insert ORDER BY id\")\n",
    "    \n",
    "    print(\"\\n🔍 Data Verification:\")\n",
    "    print(\"-\" * 70)\n",
    "    for i, row in enumerate(results[:3], 1):  # Show first 3 rows\n",
    "        print(f\"  Row {i}: {row['name']} = {row['value']}\")\n",
    "    print(f\"  ... ({len(results)} total rows)\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n📈 COMPARISON:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Old approach: 6 lines, {elapsed1*1000:.2f}ms\")\n",
    "    print(f\"  New approach: 1 line, {elapsed2*1000:.2f}ms\")\n",
    "    print(f\"  Code reduction: {((6-1)/6*100):.0f}% less code!\")\n",
    "    print(f\"  Both approaches: ✅ WORK CORRECTLY\")\n",
    "    \n",
    "    print(\"\\n💡 Winner: NEW APPROACH - Much simpler and equally fast!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Cleanup\n",
    "    await db.execute(\"DROP TABLE test_batch_insert\")\n",
    "    await db.close()\n",
    "\n",
    "# Run the test\n",
    "await test_executemany_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdefb9d",
   "metadata": {},
   "source": [
    "## 3.5. Test: Verify executemany() Method\n",
    "\n",
    "Before adding documents, let's test both the **old approach** (direct pool access) and the **new approach** (`executemany()` method) to verify our improvement works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85943a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 21:27:10,145 - async_mariadb_connector.core - INFO - Creating database connection pool.\n",
      "2025-10-21 21:27:10,177 - async_mariadb_connector.core - INFO - Database connection pool closed.\n",
      "2025-10-21 21:27:10,177 - async_mariadb_connector.core - INFO - Database connection pool closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 8 documents to vector store\n",
      "\n",
      "📝 Added documents with IDs: ['doc_0', 'doc_1', 'doc_2', 'doc_3', 'doc_4', 'doc_5', 'doc_6', 'doc_7']\n"
     ]
    }
   ],
   "source": [
    "async def add_sample_documents():\n",
    "    \"\"\"Add sample documents with mock embeddings.\"\"\"\n",
    "    \n",
    "    # Sample documents about MariaDB and databases\n",
    "    documents = [\n",
    "        \"MariaDB is a community-developed, commercially supported fork of MySQL.\",\n",
    "        \"MariaDB supports JSON data types for storing semi-structured data.\",\n",
    "        \"Vector databases are optimized for similarity search using embeddings.\",\n",
    "        \"RAG systems combine retrieval and generation for better LLM responses.\",\n",
    "        \"Async database operations improve performance in I/O-bound applications.\",\n",
    "        \"MariaDB offers full-text search capabilities for text indexing.\",\n",
    "        \"Embeddings are dense vector representations of text or other data.\",\n",
    "        \"LangChain simplifies building applications with language models.\",\n",
    "    ]\n",
    "    \n",
    "    # Generate mock embeddings (in production, use actual embedding model)\n",
    "    # For example: OpenAI's text-embedding-ada-002 or sentence-transformers\n",
    "    np.random.seed(42)\n",
    "    embeddings = [np.random.randn(384).tolist() for _ in documents]\n",
    "    \n",
    "    # Metadata for each document\n",
    "    metadatas = [\n",
    "        {\"category\": \"database\", \"topic\": \"mariadb\"},\n",
    "        {\"category\": \"database\", \"topic\": \"json\"},\n",
    "        {\"category\": \"ai\", \"topic\": \"vectors\"},\n",
    "        {\"category\": \"ai\", \"topic\": \"rag\"},\n",
    "        {\"category\": \"programming\", \"topic\": \"async\"},\n",
    "        {\"category\": \"database\", \"topic\": \"search\"},\n",
    "        {\"category\": \"ai\", \"topic\": \"embeddings\"},\n",
    "        {\"category\": \"ai\", \"topic\": \"langchain\"},\n",
    "    ]\n",
    "    \n",
    "    vector_store = MariaDBVectorStore()\n",
    "    \n",
    "    # Add documents\n",
    "    doc_ids = await vector_store.add_documents(\n",
    "        documents=documents,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📝 Added documents with IDs: {doc_ids}\")\n",
    "    \n",
    "    await vector_store.close()\n",
    "\n",
    "await add_sample_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6e02e",
   "metadata": {},
   "source": [
    "## 5. Example: Vector Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_similarity_search():\n",
    "    \"\"\"Test vector similarity search.\"\"\"\n",
    "    \n",
    "    vector_store = MariaDBVectorStore()\n",
    "    \n",
    "    # Mock query embedding (in production, embed the query text)\n",
    "    np.random.seed(42)\n",
    "    query_embedding = np.random.randn(384).tolist()\n",
    "    \n",
    "    print(\"🔍 Searching for similar documents...\\n\")\n",
    "    \n",
    "    results = await vector_store.similarity_search(\n",
    "        query_embedding=query_embedding,\n",
    "        k=3\n",
    "    )\n",
    "    \n",
    "    print(\"Top 3 Most Similar Documents:\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. Similarity: {result['similarity']:.4f}\")\n",
    "        print(f\"   Content: {result['content']}\")\n",
    "        print(f\"   Metadata: {result['metadata']}\")\n",
    "        print()\n",
    "    \n",
    "    await vector_store.close()\n",
    "\n",
    "await test_similarity_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fac98",
   "metadata": {},
   "source": [
    "## 6. Example: Hybrid Search (Full-Text + Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_hybrid_search():\n",
    "    \"\"\"Test hybrid search combining full-text and vector similarity.\"\"\"\n",
    "    \n",
    "    vector_store = MariaDBVectorStore()\n",
    "    \n",
    "    query_text = \"MariaDB database features\"\n",
    "    \n",
    "    # Mock query embedding\n",
    "    np.random.seed(123)\n",
    "    query_embedding = np.random.randn(384).tolist()\n",
    "    \n",
    "    print(f\"🔍 Hybrid Search Query: '{query_text}'\\n\")\n",
    "    \n",
    "    results = await vector_store.hybrid_search(\n",
    "        query_text=query_text,\n",
    "        query_embedding=query_embedding,\n",
    "        k=3,\n",
    "        text_weight=0.4,\n",
    "        vector_weight=0.6\n",
    "    )\n",
    "    \n",
    "    print(\"Top 3 Hybrid Search Results:\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n",
    "        print(f\"   Text Score: {result['text_score']:.4f}\")\n",
    "        print(f\"   Vector Score: {result['vector_score']:.4f}\")\n",
    "        print(f\"   Content: {result['content']}\")\n",
    "        print()\n",
    "    \n",
    "    await vector_store.close()\n",
    "\n",
    "await test_hybrid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c498c",
   "metadata": {},
   "source": [
    "## 7. RAG Pipeline: Retrieval + Generation\n",
    "\n",
    "This example shows how to integrate the vector store with an LLM for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14426d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rag_pipeline_example():\n",
    "    \"\"\"\n",
    "    Example RAG pipeline: Retrieve relevant docs and generate answer.\n",
    "    \n",
    "    Note: This is a mock example. In production:\n",
    "    1. Use a real embedding model (OpenAI, HuggingFace, etc.)\n",
    "    2. Use a real LLM for generation (GPT-4, Claude, Llama, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    vector_store = MariaDBVectorStore()\n",
    "    \n",
    "    # User question\n",
    "    question = \"What is MariaDB good for?\"\n",
    "    \n",
    "    print(f\"❓ Question: {question}\\n\")\n",
    "    \n",
    "    # Step 1: Generate query embedding (mock)\n",
    "    np.random.seed(42)\n",
    "    query_embedding = np.random.randn(384).tolist()\n",
    "    \n",
    "    # Step 2: Retrieve relevant documents\n",
    "    print(\"📚 Retrieving relevant documents...\\n\")\n",
    "    relevant_docs = await vector_store.similarity_search(\n",
    "        query_embedding=query_embedding,\n",
    "        k=3\n",
    "    )\n",
    "    \n",
    "    # Step 3: Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}: {doc['content']}\"\n",
    "        for i, doc in enumerate(relevant_docs)\n",
    "    ])\n",
    "    \n",
    "    print(\"Retrieved Context:\")\n",
    "    print(context)\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Generate answer using LLM (mock)\n",
    "    # In production, use:\n",
    "    # from langchain_openai import ChatOpenAI\n",
    "    # llm = ChatOpenAI(model=\"gpt-4\")\n",
    "    # answer = llm.invoke(prompt)\n",
    "    \n",
    "    mock_answer = (\n",
    "        \"Based on the retrieved documents, MariaDB is a community-developed fork of MySQL \"\n",
    "        \"that supports JSON data types for semi-structured data and offers full-text search \"\n",
    "        \"capabilities. It's well-suited for applications requiring both traditional relational \"\n",
    "        \"database features and modern data handling capabilities.\"\n",
    "    )\n",
    "    \n",
    "    print(\"🤖 Generated Answer:\")\n",
    "    print(mock_answer)\n",
    "    print()\n",
    "    \n",
    "    await vector_store.close()\n",
    "    \n",
    "    print(\"\\n✅ RAG pipeline completed!\")\n",
    "    print(\"\\nTo use with real LLM:\")\n",
    "    print(\"  1. Install: pip install langchain-openai\")\n",
    "    print(\"  2. Set OPENAI_API_KEY environment variable\")\n",
    "    print(\"  3. Replace mock LLM with: ChatOpenAI(model='gpt-4')\")\n",
    "\n",
    "await rag_pipeline_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c699676",
   "metadata": {},
   "source": [
    "## 8. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a88567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete all documents\n",
    "# async def cleanup():\n",
    "#     db = AsyncMariaDB()\n",
    "#     await db.execute(\"TRUNCATE TABLE document_embeddings\")\n",
    "#     await db.close()\n",
    "#     print(\"🗑️ All documents deleted\")\n",
    "# \n",
    "# await cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf89018",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ **Setting up MariaDB for vector storage** - JSON columns for embeddings\n",
    "2. ✅ **Async vector store implementation** - Compatible with LangChain\n",
    "3. ✅ **Similarity search** - Cosine similarity for semantic retrieval\n",
    "4. ✅ **Hybrid search** - Combining full-text and vector search\n",
    "5. ✅ **RAG pipeline** - Retrieval + Generation workflow\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "- **Embeddings**: Use real models (OpenAI, Cohere, HuggingFace)\n",
    "- **Indexing**: For large datasets, add database indexes\n",
    "- **Pagination**: Implement pagination for large result sets\n",
    "- **Caching**: Cache frequently accessed embeddings\n",
    "- **Monitoring**: Add logging and metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Install LangChain: `pip install langchain langchain-openai`\n",
    "- Get API keys for embedding models (OpenAI, Cohere, etc.)\n",
    "- Replace mock embeddings with real embedding generation\n",
    "- Deploy to production with proper connection pooling\n",
    "\n",
    "**Happy building! 🚀**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
