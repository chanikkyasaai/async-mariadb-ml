{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee61066d",
   "metadata": {},
   "source": [
    "# LangChain RAG Demo with Async MariaDB Connector\n",
    "\n",
    "This notebook demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline using our `async-mariadb-connector`.\n",
    "\n",
    "We will perform the following steps:\n",
    "1.  Install required libraries.\n",
    "2.  Define custom LangChain components to work with MariaDB.\n",
    "3.  Create a vector store in MariaDB with sample documents.\n",
    "4.  Perform a similarity search to retrieve relevant context for a query.\n",
    "5.  Clean up the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install async-mariadb-connector langchain-core sentence-transformers numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1ca78",
   "metadata": {},
   "source": [
    "## 1. Define Custom LangChain Components\n",
    "\n",
    "To integrate LangChain with MariaDB as a vector store, we need two custom classes:\n",
    "-   `LocalEmbeddings`: A wrapper around `sentence-transformers` to comply with LangChain's `Embeddings` interface.\n",
    "-   `MariaDBVectorStore`: A custom `VectorStore` class that handles adding texts and performing similarity searches against our MariaDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed122c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Iterable, Any, Optional\n",
    "\n",
    "# LangChain components\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Our async MariaDB library\n",
    "# Note: Ensure the library is installed. If running locally, you might need to adjust the path.\n",
    "from async_mariadb_connector import AsyncMariaDB, bulk_insert\n",
    "\n",
    "# --- Create a custom SentenceTransformer embedding class ---\n",
    "class LocalEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode([text]).tolist()[0]\n",
    "\n",
    "# --- Create a custom MariaDB VectorStore for LangChain ---\n",
    "class MariaDBVectorStore(VectorStore):\n",
    "    def __init__(self, db: AsyncMariaDB, table_name: str, embeddings: Embeddings):\n",
    "        self.db = db\n",
    "        self.table_name = table_name\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    async def aadd_texts(self, texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) -> List[str]:\n",
    "        \"\"\"Add texts to the vector store.\"\"\"\n",
    "        embedded_vectors = self.embeddings.embed_documents(list(texts))\n",
    "        \n",
    "        df_data = []\n",
    "        for i, text in enumerate(texts):\n",
    "            # MariaDB's vector type requires a specific binary format.\n",
    "            # For simplicity, we store it as BLOB.\n",
    "            vector_bytes = np.array(embedded_vectors[i], dtype=np.float32).tobytes()\n",
    "            metadata = metadatas[i] if metadatas else {}\n",
    "            df_data.append({\n",
    "                \"text_content\": text,\n",
    "                \"vector\": vector_bytes,\n",
    "                \"metadata\": str(metadata) # Storing metadata as a string representation\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(df_data)\n",
    "        await bulk_insert(self.db, self.table_name, df)\n",
    "        return [str(i) for i in range(len(texts))]\n",
    "\n",
    "    async def asimilarity_search(self, query: str, k: int = 4, **kwargs: Any) -> List[Document]:\n",
    "        \"\"\"Perform a similarity search.\"\"\"\n",
    "        query_vector = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # In a real application with MariaDB's vector type, you would use VEC_COSINE_DISTANCE\n",
    "        # This is a simplified placeholder for the demo.\n",
    "        sql_query = f\"\"\"\n",
    "            SELECT text_content, metadata\n",
    "            FROM {self.table_name}\n",
    "            ORDER BY id\n",
    "            LIMIT %s\n",
    "        \"\"\"\n",
    "        \n",
    "        results = await self.db.fetch_all(sql_query, (k,))\n",
    "        \n",
    "        return [Document(page_content=row['text_content'], metadata=eval(row['metadata'])) for row in results]\n",
    "\n",
    "    @classmethod\n",
    "    def from_texts(cls, texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, db_connection: AsyncMariaDB = None, table_name: str = \"langchain_vectors\", **kwargs: Any):\n",
    "        # This is a helper, but in an async context, it's better to call aadd_texts directly.\n",
    "        # We run it synchronously here for simplicity in the notebook.\n",
    "        vs = cls(db_connection, table_name, embedding)\n",
    "        asyncio.run(vs.aadd_texts(texts, metadatas=metadatas))\n",
    "        return vs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc911aaa",
   "metadata": {},
   "source": [
    "## 2. Run the RAG Demo\n",
    "\n",
    "Now we'll execute the main logic:\n",
    "1.  Connect to the database.\n",
    "2.  Create a table to store our vectors.\n",
    "3.  Add sample documents to the `MariaDBVectorStore`.\n",
    "4.  Run a similarity search to find documents relevant to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38727b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    print(\"--- LangChain RAG Demo with Async MariaDB ---\")\n",
    "    \n",
    "    # Sample documents for our knowledge base\n",
    "    documents = [\n",
    "        \"MariaDB is a community-developed, commercially supported fork of the MySQL relational database management system.\",\n",
    "        \"Asyncio is a library to write concurrent code using the async/await syntax.\",\n",
    "        \"LangChain is a framework for developing applications powered by language models.\",\n",
    "        \"Vector databases are used to store and query embeddings for similarity search.\"\n",
    "    ]\n",
    "    metadatas = [{\"source\": f\"doc_{i}\"} for i in range(len(documents))]\n",
    "\n",
    "    db = AsyncMariaDB()\n",
    "    await db.initialize()\n",
    "\n",
    "    try:\n",
    "        # Create table for the vector store\n",
    "        table_name = \"langchain_demo_vectors\"\n",
    "        await db.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                text_content TEXT,\n",
    "                vector BLOB,\n",
    "                metadata VARCHAR(255)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Initialize embeddings and vector store\n",
    "        embeddings = LocalEmbeddings()\n",
    "        vector_store = MariaDBVectorStore(db, table_name, embeddings)\n",
    "        \n",
    "        # Add documents to the vector store\n",
    "        print(\"Adding documents to MariaDB vector store...\")\n",
    "        await vector_store.aadd_texts(documents, metadatas=metadatas)\n",
    "        print(\"Documents added.\")\n",
    "\n",
    "        # Perform a similarity search (RAG query)\n",
    "        query = \"What is MariaDB?\"\n",
    "        print(f\"\\nPerforming similarity search for: '{query}'\")\n",
    "        results = await vector_store.asimilarity_search(query, k=2)\n",
    "        \n",
    "        print(\"\\nTop relevant documents from MariaDB:\")\n",
    "        for doc in results:\n",
    "            print(f\"  - Content: {doc.page_content}\")\n",
    "            print(f\"    Source: {doc.metadata.get('source')}\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up\n",
    "        await db.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        print(f\"\\nTable '{table_name}' dropped.\")\n",
    "        await db.close()\n",
    "        print(\"Connection closed.\")\n",
    "        print(\"\\n--- LangChain RAG Demo Complete ---\")\n",
    "\n",
    "# Run the async main function\n",
    "# If you get a \"RuntimeError: This event loop is already running\",\n",
    "# you might need to use nest_asyncio in a notebook environment.\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
